{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "mendnowq2ozdjagonjiy",
   "authorId": "4693832293054",
   "authorName": "NICKYNIRAV",
   "authorEmail": "nickynirav@gmail.com",
   "sessionId": "dc6043be-01b7-4101-8009-bdec2f4271ee",
   "lastEditTime": 1747723113224
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcff1151-4d2f-461d-bfa6-937eeb234bd1",
   "metadata": {
    "name": "cell6",
    "collapsed": false
   },
   "source": "# üõçÔ∏è Retail Product Recommender System using SnowparkML\n\n## üìã Overview\nThis notebook demonstrates an end-to-end machine learning pipeline for building a product recommendation system using Snowpark ML. The system analyzes retail transaction data to generate personalized product recommendations using Bayesian Personalized Ranking.\n\n## üîÑ Pipeline Components\n1. **üîç Data Loading & Preparation**: Extract retail transaction data from Snowflake stage\n2. **üìä Data Exploration & Analysis**: Understand customer purchase patterns and product relationships\n3. **‚öôÔ∏è Feature Engineering**: Convert transaction data into user-item interaction matrices\n4. **üß† Model Training**: Implement BPR algorithm with hyperparameter tuning\n5. **üìè Model Evaluation**: Calculate precision, recall, and coverage metrics\n6. **üöÄ Model Deployment**: Register model for real-time recommendations\n7. **üìà Performance Visualization**: Interactive dashboards for model insights\n\n## üíº Business Value\nThis recommendation system enables retailers to:\n- üí∞ Increase average order value through personalized product suggestions\n- ü§ù Enhance customer experience with relevant recommendations\n- üîÑ Identify cross-selling opportunities based on purchase patterns\n- üì¶ Optimize inventory management based on predicted demand"
  },
  {
   "cell_type": "code",
   "id": "1f6265a7-c45d-4f7f-8c41-00684e61935c",
   "metadata": {
    "language": "python",
    "name": "initialize_environment",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "\"\"\"\nRetail Recommender System - Streamlit App\nSets up the environment for product recommendations using Snowflake and Streamlit.\n\"\"\"\n\n# Core data and visualization libraries\nimport streamlit as st             # Web app framework for interactive UI\nimport pandas as pd                # Data manipulation and analysis\nimport numpy as np                 # Numerical operations\nimport matplotlib.pyplot as plt    # Base plotting library\nimport seaborn as sns              # Statistical data visualization\nfrom datetime import datetime, timedelta  # Date handling\nimport warnings                    # Warning control\nimport joblib                      # Model persistence\nimport time                        # Time utilities for performance tracking\nimport sys\n\n\n# Snowflake libraries\nfrom snowflake.snowpark.context import get_active_session  # Access current Snowflake connection\nfrom snowflake.snowpark.types import StringType, IntegerType  # Snowflake data types\nimport snowflake.snowpark.functions as F  # Snowflake SQL functions\nfrom snowflake.ml.registry import Registry  # Model registry for MLOps\nfrom snowflake.ml.modeling.pipeline import Pipeline  # ML pipeline management\nimport snowflake.ml.modeling.preprocessing as pp  # Snowflake-specific preprocessing\nfrom snowflake.ml.modeling.xgboost import XGBClassifier  # Gradient boosting implementation\nfrom IPython.display import Markdown, display  # Rich output display\nfrom implicit.bpr import BayesianPersonalizedRanking  # Recommendation algorithm\nfrom sklearn.model_selection import train_test_split  # Dataset splitting\nfrom matplotlib.ticker import FuncFormatter  # Custom axis formatting\n\n# Configure pandas display settings\npd.set_option('display.max_columns', None)  # Show all columns\npd.set_option('display.width', 1000)        # Set wide display width\npd.set_option('display.max_colwidth', None) # Show full content of each cell\nwarnings.filterwarnings('ignore')           # Suppress warning messages\n\n# ML and analytics libraries\nfrom scipy.sparse import coo_matrix  # Sparse matrix for recommendation models\nfrom sklearn.preprocessing import LabelEncoder  # Convert categorical variables\n\ndef setup_snowflake_session():\n    \"\"\"Configure Snowflake session with tracking tags\"\"\"\n    session = get_active_session()  # Get current Snowflake session\n    # Add metadata tags for tracking and governance\n    session.query_tag = {\n        \"origin\": \"sf_demo-retail-recommender\",\n        \"name\": \"mlops_product_recommendation\",\n        \"version\": {\"major\": 1, \"minor\": 0}\n    }\n    return session\n\ndef configure_environment():\n    \"\"\"Set up visualization and pandas configuration\"\"\"\n    # Configure pandas and suppress warnings\n    pd.set_option(\"display.max_columns\", 500)  # Ensure we see all columns in outputs\n    pd.set_option(\"display.max_rows\", 10)      # Limit default row display\n    pd.options.mode.chained_assignment = None  # Disable chained assignment warning\n    warnings.filterwarnings('ignore', category=FutureWarning)  # Suppress future warnings\n    warnings.filterwarnings('ignore', category=UserWarning, message='.*tight_layout.*')  # Suppress layout warnings\n    \n    # Set visualization style\n    plt.style.use('seaborn-v0_8-whitegrid')  # Clean, professional visualization style\n    return sns.color_palette(\"viridis\", n_colors=10)  # Return color palette for consistent visuals\n\ndef main():\n    \"\"\"Initialize session and environment\"\"\"\n    session = setup_snowflake_session()  # Configure Snowflake connection\n    custom_palette = configure_environment()  # Set up visualization settings\n    solution_prefix = session.get_current_warehouse()  # Get warehouse name for reference\n    return session, solution_prefix, custom_palette\n\n# Execute setup based on context\nif __name__ == \"__main__\":\n    # Direct execution path (when run as a script)\n    session, solution_prefix, custom_palette = main()\n    st.write(f\"Connected to warehouse: {solution_prefix}\")  # Display connection info in Streamlit\nelse:\n    # Import path (when imported as a module)\n    session = setup_snowflake_session()\n    custom_palette = configure_environment()\n    solution_prefix = session.get_current_warehouse()\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1d3b6be1-3da2-427c-b3ce-1993c355bbea",
   "metadata": {
    "name": "cell8",
    "collapsed": false
   },
   "source": "## üõí Snowflake Environment Setup\n\nThis SQL block configures the necessary Snowflake components for our retail recommender system:\n\n- Creates a custom CSV file format that skips header rows\n- Establishes a dedicated storage stage for retail data files\n- Includes reference commands for uploading CSV data files\n\nThe setup provides a standardized environment for consistent data ingestion and processing throughout our recommendation pipeline."
  },
  {
   "cell_type": "code",
   "id": "644af0e0-d7b2-4ca4-9db7-5bcfea3516ff",
   "metadata": {
    "language": "sql",
    "name": "setup_stage_format",
    "codeCollapsed": false,
    "collapsed": false
   },
   "outputs": [],
   "source": "-- Set up data format and stage for retail recommender\n-- Define CSV format for ingesting retail data\nCREATE FILE FORMAT IF NOT EXISTS CSVFORMAT \n    SKIP_HEADER = 1   -- Skip header row in CSV files\n    TYPE = 'CSV';     -- Use CSV format for data ingestion\n    \n-- Create stage to store and access data files\nCREATE OR REPLACE STAGE snowflake_recommender_system_with_implicit_bpr_stage\n    FILE_FORMAT = (TYPE = 'CSV');\n\n-- Commented options for file upload to stage:\n-- PUT file:///online_retail_2010.csv snowflake_recommender_system_with_implicit_bpr_stage;\n-- PUT file://online_retail_2010.csv @snowflake_recommender_system_with_implicit_bpr_stage AUTO_COMPRESS=FALSE;\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "266b53aa-b9e9-40bd-9244-2a0639cd67bd",
   "metadata": {
    "name": "cell56",
    "collapsed": false
   },
   "source": "## üìä Data Preparation and Staging\n\nThis code block handles the initial data preparation by:\n\n1. Importing our retail dataset from an Excel file containing multiple sheets\n2. Consolidating all worksheets into a single unified DataFrame\n3. Converting the consolidated data to CSV format for compatibility with Snowflake\n4. Uploading the prepared dataset to our previously configured Snowflake stage\n\nThis step bridges the gap between raw source data and our cloud-based analytics environment, making the retail transaction history available for subsequent processing and model building steps."
  },
  {
   "cell_type": "code",
   "id": "70af9ce6-ee4f-46ea-b63a-26ee8300c06f",
   "metadata": {
    "language": "python",
    "name": "upload_excel_data",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Process Excel data and upload to Snowflake stage\nall_sheets = pd.read_excel(\"online_retail_II.xlsx\", sheet_name=None)  # Load all sheets\ncombined_df = pd.concat(all_sheets.values(), ignore_index=True)       # Merge sheets into one dataframe\n\n# Export to CSV and upload to Snowflake\ncombined_df.to_csv(\"online_retail_II.csv\", index=False)               # Convert to CSV format\nsession.file.put('online_retail_II.csv', \"@snowflake_recommender_system_with_implicit_bpr_stage\", auto_compress=False)  # Upload to stage",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "594f9f89-5d04-431b-93b0-6962b88ee484",
   "metadata": {
    "name": "cell12",
    "collapsed": false
   },
   "source": "## üìÇ Stage Content Verification\n\nThis simple SQL command lists all files currently stored in our `retail_recommender_stage` storage location. Running this command helps verify that our data upload was successful and allows us to confirm the availability of required datasets before proceeding with data processing and model development."
  },
  {
   "cell_type": "code",
   "id": "b5d8c032-376d-435b-9e11-76a5b5d23b7b",
   "metadata": {
    "language": "sql",
    "name": "list_stage_files",
    "codeCollapsed": false,
    "collapsed": false
   },
   "outputs": [],
   "source": "LS @snowflake_recommender_system_with_implicit_bpr_stage;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f5dcd3fc-6ab0-44fd-a5ed-c012fb3f9e8e",
   "metadata": {
    "name": "cell58",
    "collapsed": false
   },
   "source": "## üì• Data Loading and Preview\n\nThis code block transitions our data from raw files in the Snowflake stage to a structured Snowpark DataFrame for analysis:\n\n1. Reads the CSV data with appropriate configuration for delimiters and encoding\n2. Enables schema inference to automatically detect appropriate data types\n3. Properly handles the CSV header to establish column names\n4. Displays a formatted preview of the first 10 rows to verify data quality\n\nThis step establishes our working dataset and allows us to visually confirm the structure and content before moving forward with data cleaning and feature engineering."
  },
  {
   "cell_type": "code",
   "id": "004cc231-0b36-4c07-b88d-7d266a144ecc",
   "metadata": {
    "language": "python",
    "name": "load_stage_data",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Load CSV data from stage into Snowpark dataframe\nspdf = session.read.options({\n    \"field_delimiter\": \",\",            # Define comma as separator\n    \"field_optionally_enclosed_by\": '\"',  # Allow quotes around fields\n    \"infer_schema\": True,              # Automatically detect data types\n    \"parse_header\": True               # Use first row as column names\n}).csv(\"@snowflake_recommender_system_with_implicit_bpr_stage\")    # Read from Snowflake stage\n\n# Display a formatted preview of the dataframe\nprint(\"\\n=== ONLINE RETAIL DATASET PREVIEW ===\")\nspdf.show(10)                          # Show 10 rows of the dataframe\nprint(\"=============================\\n\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6ab07206-44a4-4da0-a93e-253bf13fb315",
   "metadata": {
    "name": "cell18",
    "collapsed": false
   },
   "source": "## üî¢ Data Type Conversion for ML Compatibility\n\nThis code block performs targeted data type conversions to ensure our dataset works properly with machine learning algorithms. It systematically identifies decimal columns and converts them to double-precision floating-point format, addressing a common compatibility issue when working with Snowflake data in ML pipelines."
  },
  {
   "cell_type": "code",
   "id": "c1bbb34f-4889-4529-ab49-d5dac7dfbd5b",
   "metadata": {
    "language": "python",
    "name": "decimal_conversion",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Convert decimal columns to double for ML compatibility\nfrom snowflake.snowpark.types import DecimalType, DoubleType  # Import required Snowflake data types\n\n# Find and convert all decimal columns in one pass\nfor col_name in [f.name for f in spdf.schema.fields if isinstance(f.datatype, DecimalType)]:  # Identify all decimal columns dynamically\n    spdf = spdf.with_column(col_name, spdf[col_name].cast(DoubleType()))  # Cast each decimal column to double type",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ca570eeb-0197-4271-a10c-cea705c16167",
   "metadata": {
    "name": "cell19",
    "collapsed": false
   },
   "source": "## üè∑Ô∏è Column Standardization and DataFrame Conversion\n\nThis code block handles the critical task of normalizing column names and converting our data to a pandas DataFrame. It removes special characters and quotes from column names while ensuring consistent naming conventions, which simplifies subsequent code development and improves readability."
  },
  {
   "cell_type": "code",
   "id": "987afd10-effa-4368-9747-477da7bb125a",
   "metadata": {
    "language": "python",
    "name": "column_normalization",
    "codeCollapsed": false,
    "collapsed": false
   },
   "outputs": [],
   "source": "# Convert Snowpark DataFrame to pandas with clean column names\ncolumns_mapping = {\n    '\"Customer ID\"': \"CustomerID\",     # Remove quotes and space from customer identifier\n    '\"StockCode\"': \"StockCode\",        # Standardize product code column name\n    '\"Description\"': \"Description\",    # Clean product description column name\n    '\"Price\"': \"Price\",                # Normalize price column name\n    '\"Quantity\"': \"Quantity\",          # Standardize quantity column name\n    '\"Invoice\"': \"Invoice\",            # Clean invoice identifier column name\n    '\"InvoiceDate\"': \"InvoiceDate\",    # Normalize transaction date column name\n    '\"Country\"': \"Country\"             # Standardize country column name\n}\n\n# Extract selected columns and rename in one operation\nonline_retail_data = spdf[list(columns_mapping.keys())].to_pandas()  # Select columns and convert to pandas\nonline_retail_data.columns = list(columns_mapping.values())          # Apply clean column names\n\nonline_retail_data",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fba566f6-816d-4b3e-9323-8fa87ac8ff38",
   "metadata": {
    "name": "cell5",
    "collapsed": false
   },
   "source": "## üõí Customer Purchase History Display Function\n\nThis markdown describes a utility function that retrieves and displays purchase history for a specific customer. The function includes robust error handling for different CustomerID data types and provides attractive formatting to highlight key transaction details."
  },
  {
   "cell_type": "code",
   "id": "21ccc186-b8f0-4871-92e3-2681db4e2c66",
   "metadata": {
    "language": "python",
    "name": "purchase_history_display",
    "codeCollapsed": false,
    "collapsed": false
   },
   "outputs": [],
   "source": "def display_original_purchase_history(df, customer_id, limit=5):\n    \"\"\"\n    Show top purchases for a specific customer with attractive formatting\n    \n    Parameters:\n    -----------\n    df : DataFrame\n        The retail dataset containing purchase history\n    customer_id : int or str\n        The customer ID to search for\n    limit : int, default=5\n        Maximum number of purchases to display\n        \n    Returns:\n    --------\n    DataFrame or styled DataFrame\n        The formatted purchase history\n    \"\"\"\n    # Handle different CustomerID types\n    try:\n        # Convert CustomerID for comparison based on DataFrame's type\n        df_type = type(df[\"CustomerID\"].iloc[0])\n        if df_type != type(customer_id):\n            if isinstance(df[\"CustomerID\"].iloc[0], (int, np.integer)):\n                customer_id = int(customer_id)  # Convert to integer if DataFrame uses integers\n            elif isinstance(df[\"CustomerID\"].iloc[0], (float, np.float64)):  # Updated to np.float64\n                customer_id = float(customer_id)  # Convert to float if DataFrame uses floats\n            elif isinstance(df[\"CustomerID\"].iloc[0], str):\n                customer_id = str(customer_id)  # Convert to string if DataFrame uses strings\n    except (IndexError, TypeError):\n        # If type checking fails, continue with original customer_id\n        pass\n    \n    # Try both direct and string-based comparison\n    purchases = df[df[\"CustomerID\"] == customer_id]  # First attempt direct comparison\n    \n    if purchases.empty:\n        # Try string comparison if direct comparison failed\n        purchases = df[df[\"CustomerID\"].astype(str) == str(customer_id)]  # Fallback to string comparison\n    \n    # Select relevant columns if we found matches\n    if not purchases.empty:\n        purchases = purchases[\n            [\"CustomerID\", \"StockCode\", \"Description\", \"Quantity\", \"Invoice\", \"InvoiceDate\"]\n        ]  # Keep only the most relevant transaction details\n    \n    # Return empty dataframe with message if customer has no purchases\n    if purchases.empty:\n        print(f\"‚ö†Ô∏è No purchase history found for Customer ID: {customer_id}\")\n        return purchases\n    \n    # Display styled header\n    print(f\"\\nüõí Purchase History for Customer ID: {customer_id}\")\n    print(f\"Showing top {limit} of {len(purchases)} items\")\n    print(\"---------------------------------------------\")\n    \n    # Format and display the dataframe with styling\n    try:\n        styled_df = purchases.head(limit).style.set_properties(**{\n            'background-color': '#f5f5f5',  # Light gray background for readability\n            'border-color': '#888888',      # Medium gray borders for definition\n            'font-size': '11pt'             # Consistent font size\n        }).format({\n            'Quantity': '{:,.0f}',          # Format quantities as integers\n            'InvoiceDate': '{:%Y-%m-%d}'    # Format dates consistently\n        })\n        \n        # Handle potential hide_index compatibility issue with version check\n        import pandas as pd\n        if pd.__version__ >= '1.4.0':\n            styled_df = styled_df.hide(axis=\"index\")  # New method in pandas 1.4+\n        else:\n            try:\n                styled_df = styled_df.hide_index()  # Older pandas versions\n            except:\n                pass  # Very old pandas version, skip hiding index\n            \n        return styled_df\n    except Exception as e:\n        print(f\"Note: Basic formatting applied due to styling error: {e}\")\n        # Fallback to plain DataFrame if styling fails\n        return purchases.head(limit)  # Return basic dataframe if styling fails",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "57a1183d-e214-45e2-b4e0-f3ef17937c10",
   "metadata": {
    "name": "cell11",
    "collapsed": false
   },
   "source": "## üìä Alternative Data Loading Method (Direct Excel Import)\n\nThis cell demonstrates an alternative approach to load retail data directly from Excel files without using Snowflake stages. This method is useful when working locally or when you need to process the Excel file before uploading to Snowflake."
  },
  {
   "cell_type": "code",
   "id": "5004e7f8-a247-4c14-b090-f3e7259fbb51",
   "metadata": {
    "language": "python",
    "name": "direct_excel_import",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "'''# This would take some time as we would be filling close 1 millon transactions from the downloaded file and creating a dataframe\nraw_excel_data = pd.read_excel(\"online_retail_II.xlsx\",\n    sheet_name=None,                 # Load all sheets in the Excel file\n    engine='openpyxl',               # Use the openpyxl engine for Excel processing \n    names=[                          # Define column names explicitly for consistency\n        \"Invoice\",\n        \"StockCode\",\n        \"Description\",\n        \"Quantity\",\n        \"InvoiceDate\",\n        \"Price\",\n        \"CustomerID\",\n        \"Country\",\n    ],\n)\n\n# Since the file contains different two sheets from two years, let us combine and create a single dataframe\nonline_retail_data = pd.concat(raw_excel_data, axis=0, ignore_index=True)  # Merge all sheets into one dataset\n\n# Print the recently created Panda Dataset's dimensionality and first five records\nonline_retail_data = online_retail_data[\n    [                               # Reorder columns for better readability and consistency\n        \"CustomerID\",\n        \"StockCode\",\n        \"Description\",\n        \"Price\",\n        \"Quantity\",\n        \"Invoice\",\n        \"InvoiceDate\",\n        \"Country\",\n    ]\n]\n'''",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9a73bb46-dcf9-4612-b4da-baf2a8fb753e",
   "metadata": {
    "name": "cell2",
    "collapsed": false
   },
   "source": "## üìù Product Lookup Table Creation\n\nThis cell builds a reference lookup table that maps product codes to descriptions. This essential component allows our recommendation system to translate cryptic product codes into human-readable descriptions when displaying results to users."
  },
  {
   "cell_type": "code",
   "id": "5c8367dd-7ad9-477c-914c-5320d7e12648",
   "metadata": {
    "language": "python",
    "name": "product_lookup_creation",
    "codeCollapsed": false,
    "collapsed": false
   },
   "outputs": [],
   "source": "# Create product lookup table for recommendation display\nstock_code_desc_look_up = (online_retail_data[[\"StockCode\", \"Description\"]]  # Extract only product code and description columns\n                          .drop_duplicates()                                 # Remove duplicate products\n                          .astype({\"StockCode\": str}))                       # Ensure consistent string format for codes\n\nprint(f\"[INFO]: Stock Code lookup table created with {len(stock_code_desc_look_up):,} unique products\")\n\n# Display sample with basic styling (compatible with older pandas versions)\npreview_df = stock_code_desc_look_up.head(3)                                 # Get first 3 products for preview\ntry:\n    # For newer pandas versions\n    display(preview_df.style.set_properties(**{'background-color': '#f5f5f5'}).hide())  # Apply subtle background styling\nexcept AttributeError:\n    # Fallback for older pandas versions\n    display(preview_df.style.set_properties(**{'background-color': '#f5f5f5'}))         # Simplified styling for compatibility",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d6ae1b2d-c100-4315-a1e2-6220d420e341",
   "metadata": {
    "name": "cell3",
    "collapsed": false
   },
   "source": "## üé® Data Visualization Utility Function\n\nThis cell defines a custom styling function that transforms plain dataframes into visually appealing tables. The function applies professional formatting to dates, currencies, and numbers while ensuring compatibility across different pandas versions."
  },
  {
   "cell_type": "code",
   "id": "b1136cd4-f9dd-4902-9215-1b2583bc7992",
   "metadata": {
    "language": "python",
    "name": "styled_data_preview",
    "codeCollapsed": false,
    "collapsed": false
   },
   "outputs": [],
   "source": "# Display a polished preview of the retail data\ndef styled_preview(df, rows=5):\n    \"\"\"Create an elegantly styled preview of the dataframe\"\"\"\n    # Make a copy to avoid modifying the original\n    preview_df = df.head(rows).copy()\n    \n    # Format currency without using \\\\$ in the formatter\n    if \"Price\" in preview_df.columns:\n        preview_df[\"Price\"] = preview_df[\"Price\"].apply(lambda x: f\"¬£{x:.2f}\" if pd.notnull(x) else \"\")  # Format prices as British pounds\n    \n    # Format specifications for other column types\n    format_dict = {\n        \"CustomerID\": \"{:,.0f}\",         # Whole numbers without decimals\n        \"Quantity\": \"{:,.0f}\",           # Whole numbers without decimals\n        \"InvoiceDate\": \"{:%Y-%m-%d}\"     # Formatted dates\n    }\n    \n    # Apply styling with modern design\n    try:\n        styled = preview_df.style\\\n            .format(format_dict)\\\n            .set_properties(**{\n                'font-family': 'Segoe UI, Helvetica, Arial, sans-serif',  # Professional font family\n                'text-align': 'left',                                      # Left-aligned for better readability\n                'border': '1px solid #e0e0e0',                             # Subtle borders\n                'padding': '8px',                                          # Comfortable padding\n                'background-color': '#ffffff'                              # Clean white background\n            })\\\n            .set_table_styles([\n                {'selector': 'thead th', 'props': [\n                    ('background-color', '#f2f2f2'),                       # Light gray header background\n                    ('color', '#333333'),                                  # Dark text for contrast\n                    ('font-weight', 'bold'),                               # Bold headers\n                    ('border-bottom', '2px solid #cccccc')                 # Distinct header border\n                ]},\n                {'selector': 'tbody tr:nth-child(even)', 'props': [\n                    ('background-color', '#f9f9f9')                        # Zebra striping for rows\n                ]}\n            ])\n        \n        # Try to hide index - handle different pandas versions\n        try:\n            return styled.hide(axis='index')                               # Hide index in newer pandas versions\n        except AttributeError:\n            return styled                                                  # Fallback for older versions\n    except Exception as e:\n        # Fallback to simpler styling if advanced styling fails\n        print(f\"Using simplified styling due to: {str(e)}\")\n        return preview_df                                                  # Return plain preview if styling fails\n\n# Display the styled preview\ndisplay(styled_preview(online_retail_data))                                # Show formatted retail data",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e687c198-517e-4ce7-a55e-88dcaf7e5be8",
   "metadata": {
    "name": "cell16",
    "collapsed": false
   },
   "source": "## üìä Dataset Profile and Quality Assessment\n\nThis cell performs a comprehensive examination of our retail dataset structure, displaying key metrics like record count, data types, memory usage, and missing values. These insights help us understand data quality issues before proceeding with analysis\n"
  },
  {
   "cell_type": "code",
   "id": "2af42f36-3554-44e9-ba11-e74261544245",
   "metadata": {
    "language": "python",
    "name": "data_quality_assessment",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Display dataset structure and quality metrics\nprint(\"\\n\" + \"=\"*50)\nprint(\"üìã RETAIL DATASET PROFILE\".center(50))\nprint(\"=\"*50)\n\n# Show dataset dimensions with formatting\nrecord_count = f\"{len(online_retail_data):,}\"\nfeature_count = f\"{online_retail_data.shape[1]}\"\nprint(f\"üìù Records: {record_count} | üî¢ Features: {feature_count}\")\n\n# Create formatted table for data types and memory usage\nprint(\"\\n\" + \"üìä DATATYPES AND MEMORY USAGE\".center(50))\nprint(\"-\"*50)\nprint(f\"{'Column':<15} {'Type':<12} {'Memory':<10} {'% of Total':<12}\")\nprint(\"-\"*50)\n\ntotal_memory = 0\nfor col in online_retail_data.columns:\n    mem_usage = online_retail_data[col].memory_usage(deep=True) / (1024 * 1024)\n    total_memory += mem_usage\n\nfor col, dtype in zip(online_retail_data.columns, online_retail_data.dtypes):\n    mem_usage = online_retail_data[col].memory_usage(deep=True) / (1024 * 1024)\n    pct_total = (mem_usage / total_memory) * 100\n    print(f\"{col:<15} {str(dtype):<12} {mem_usage:6.2f} MB  {pct_total:6.1f}%\")\n\n# Show missing values with visual indicators\nprint(\"\\n\" + \"‚ö†Ô∏è MISSING VALUES SUMMARY\".center(50))\nprint(\"-\"*50)\nmissing_found = False\n\nfor col in online_retail_data.columns:\n    missing = online_retail_data[col].isnull().sum()\n    if missing > 0:\n        missing_found = True\n        pct = (missing / len(online_retail_data)) * 100\n        bar_length = int(pct / 2)  # Scale for visual bar\n        bar = '‚ñà' * bar_length + '‚ñë' * (50 - bar_length)\n        print(f\"{col:<12} {missing:>8,} missing ({pct:5.1f}%) {bar}\")\n\nif not missing_found:\n    print(\"‚úÖ No missing values detected in the dataset\")\n    \n# Display quick statistics with improved formatting\nprint(\"\\n\" + \"üìà KEY STATISTICS SUMMARY\".center(50))\nprint(\"-\"*50)\nnumeric_cols = online_retail_data.select_dtypes(include=['number']).columns\n\nif len(numeric_cols) > 0:\n    stats = online_retail_data[numeric_cols].describe()\n    \n    # Format each statistic with appropriate precision\n    for col in stats.columns:\n        col_max = stats[col].max()\n        if abs(col_max) < 10:\n            # For small numbers, show more decimals\n            stats[col] = stats[col].apply(lambda x: f\"{x:.3f}\")\n        elif abs(col_max) < 1000:\n            # For medium numbers\n            stats[col] = stats[col].apply(lambda x: f\"{x:.2f}\")\n        else:\n            # For large numbers, use comma separators\n            stats[col] = stats[col].apply(lambda x: f\"{int(x):,}\" if x == int(x) else f\"{x:,.1f}\")\n    \n    print(stats.T.to_string())\nelse:\n    print(\"No numeric columns available for statistics\")\n\nprint(\"\\n\" + \"=\"*50)\n\n# Add key insights based on the data\nprint(\"\\nüîç KEY INSIGHTS:\")\nprint(f\"‚Ä¢ Dataset spans {online_retail_data['InvoiceDate'].min().date()} to {online_retail_data['InvoiceDate'].max().date()}\")\nprint(f\"‚Ä¢ Contains transactions from {online_retail_data['Country'].nunique()} different countries\")\nprint(f\"‚Ä¢ {online_retail_data['CustomerID'].nunique():,} unique customers with purchasing history\")\nprint(f\"‚Ä¢ Average purchase quantity: {online_retail_data['Quantity'].mean():.1f} items\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "66ed88dd-0ba1-407b-9c40-67badc716817",
   "metadata": {
    "name": "cell13",
    "collapsed": false
   },
   "source": "## üßπ Data Cleaning Pipeline\n\nA modular pipeline for preprocessing online retail data, built on scikit-learn. This cell implements a robust data cleaning pipeline that:\n\n    ‚úÖ Removes missing CustomerID records\n    ‚úÖ Standardizes data types for consistency\n    ‚úÖ Provides beautiful visual reporting\n    ‚úÖ Saves for production reuse\n\nThe pipeline follows ML best practices with proper train/test splitting and can deploy to both local and Snowflake environments.\n\nThe implementation produces detailed visual reports showing the cleaning impact and works seamlessly with recommendation system workflows."
  },
  {
   "cell_type": "code",
   "id": "9ad9cbfb-1a13-4b14-8c36-6004821232e8",
   "metadata": {
    "language": "python",
    "name": "data_cleanig_processing_pipeline",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "import numpy as np\nimport pandas as pd\nfrom sklearn.pipeline import Pipeline  # Back to sklearn Pipeline\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.model_selection import train_test_split\nimport joblib\nimport sys\nfrom time import sleep\n\n# Define the DataCleaner class at the module level\nclass DataCleaner(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Custom transformer for cleaning online retail data:\n    - Removes records without CustomerID\n    - Converts CustomerID to integer type\n    - Provides detailed reporting on cleaning operations\n    \"\"\"\n    \n    def __init__(self, customer_id_col='CustomerID', report=True):\n        self.customer_id_col = customer_id_col\n        self.report = report\n        self.initial_count = None\n        self.initial_customers = None\n        self.removed_count = None\n        self.final_customers = None\n        self.removal_percentage = None\n        \n    def fit(self, X, y=None):\n        # Store initial metrics for reporting\n        self.initial_count = len(X)\n        self.initial_customers = X[self.customer_id_col].dropna().nunique()\n        return self\n        \n    def transform(self, X, y=None):\n        # Make a copy to avoid modifying the original dataframe\n        df = X.copy()\n        \n        # Remove rows with missing CustomerID\n        df = df.dropna(subset=[self.customer_id_col])\n        \n        # Convert CustomerID to integer type\n        df[self.customer_id_col] = df[self.customer_id_col].astype(int)\n        \n        # Calculate result metrics\n        self.removed_count = self.initial_count - len(df)\n        self.removal_percentage = (self.removed_count / self.initial_count) * 100\n        self.final_customers = df[self.customer_id_col].nunique()\n        \n        # Generate report if enabled\n        if self.report:\n            self._print_report()\n            \n        return df\n    \n    def _print_report(self):\n        \"\"\"Prints a detailed report of the cleaning operations and their impact\"\"\"\n        print(\"\\n\" + \"‚ïê\"*70)\n        print(\"üßπ  DATA CLEANING PROCESS  üßπ\".center(70))\n        print(\"‚ïê\"*70)\n        \n        # Report initial status\n        print(f\"‚Ä¢ Initial dataset: {self.initial_count:,} transactions\")\n        print(f\"‚Ä¢ Found {self.removed_count:,} transactions without CustomerID\")\n        \n        # Report detailed results with visual indicators\n        print(\"\\nüìä CLEANING RESULTS:\")\n        print(f\"  ‚úì Removed {self.removed_count:,} incomplete records ({self.removal_percentage:.1f}% of data)\")\n        print(f\"  ‚úì Converted CustomerID to integer type for consistency\")\n        print(f\"  ‚úì Final dataset contains {self.initial_count - self.removed_count:,} valid transactions\")\n        print(f\"  ‚úì Dataset now has {self.final_customers:,} unique customers\")\n        \n        # Add a visual representation of the data cleaning effect\n        print(\"\\nüìà DATA CLEANING IMPACT:\")\n        retention_rate = (self.initial_count - self.removed_count) / self.initial_count\n        removal_rate = self.removed_count / self.initial_count\n        print(\"  \" + \"‚ñà\" * int(retention_rate * 40) + \n              \"‚ñë\" * int(removal_rate * 40) + f\" {retention_rate:.1%} retained\")\n        print(\"‚ïê\"*70 + \"\\n\")\n\n\ndef print_section_header(title, width=70):\n    \"\"\"Print a beautifully formatted section header\"\"\"\n    print(\"\\n\" + \"‚ïê\"*width)\n    print(f\"  {title}  \".center(width, \"‚ïê\"))\n    print(\"‚ïê\"*width)\n\n\ndef print_step_progress(step_name, pause=0.5):\n    \"\"\"Print a step progress indicator with a brief pause for visual effect\"\"\"\n    print(f\"\\n‚è≥ {step_name}...\", end=\"\", flush=True)\n    sleep(pause)\n    print(\" ‚úÖ\")\n\n\n# Make the class picklable by setting it as an attribute of the current module\nsetattr(sys.modules[__name__], 'DataCleaner', DataCleaner)\n\n\n# Main execution flow\nif __name__ == \"__main__\" or True:  # Always execute in notebook environment\n    # Start with a clear header\n    print_section_header(\"üîÑ RECOMMENDER SYSTEM DATA PIPELINE üîÑ\")\n    \n    # Build the preprocessing pipeline\n    print_step_progress(\"Building data processing pipeline\")\n    data_processing_pipeline = Pipeline(\n        steps=[\n            (\n                \"data_cleaner\",\n                DataCleaner(customer_id_col='CustomerID', report=False)  # Turn off immediate reporting\n            )\n        ]\n    )\n    \n    # Define the pipeline file location\n    PIPELINE_FILE = '/tmp/snowflake_implicit_bpr_recommender_processing_pipeline.joblib'\n    \n    # Split data for training/testing using scikit-learn\n    print_step_progress(\"Splitting data into training and testing sets\")\n    training, testing = train_test_split(\n        online_retail_data, test_size=0.2, random_state=111\n    )\n    \n    # Process training data\n    print_step_progress(\"Processing training data\")\n    training_processed = data_processing_pipeline.fit(training).transform(training)\n    \n    # Process testing data (correctly, without refitting)\n    print_step_progress(\"Processing testing data\") \n    testing_processed = data_processing_pipeline.transform(testing)\n    \n    # Process full dataset with reporting enabled (for visualization)\n    print(\"\\nüìã Processing full dataset with detailed reporting:\")\n    data_processing_pipeline.named_steps['data_cleaner'].report = True\n    cleaned_data = data_processing_pipeline.transform(online_retail_data)\n    online_retail_data = cleaned_data;\n    \n    # Save the pipeline\n    print_step_progress(\"Saving pipeline to local file\")\n    joblib.dump(data_processing_pipeline, PIPELINE_FILE)\n    \n    # Create stage if it doesn't exist\n    print_step_progress(\"Creating Snowflake stage\")\n    stage_name = \"snowflake_implicit_bpr_recommender_processing_pipeline_stage\"\n    \n    try:\n        stage_result = session.sql(f\"CREATE OR REPLACE STAGE {stage_name}\").collect()\n        \n        # Upload to Snowflake stage\n        print_step_progress(\"Uploading pipeline to Snowflake stage\")\n        upload_result = session.file.put(\n            PIPELINE_FILE, \n            f\"@{stage_name}\", \n            overwrite=True\n        )\n        \n        # Print upload details\n        print_section_header(\"üì§ FILE UPLOAD DETAILS üì§\")\n        \n        if upload_result:\n            # Display upload results in a formatted way\n            upload_info = upload_result[0]\n            print(f\"‚Ä¢ File name: {upload_info[0]}\")\n            print(f\"‚Ä¢ Compressed name: {upload_info[1]}\")\n            print(f\"‚Ä¢ Source size: {upload_info[2]:,} bytes\")\n            print(f\"‚Ä¢ Target size: {upload_info[3]:,} bytes\")\n            print(f\"‚Ä¢ Source compression: {upload_info[4]}\")\n            print(f\"‚Ä¢ Target compression: {upload_info[5]}\")\n            print(f\"‚Ä¢ Status: {upload_info[6]}\")\n            \n            # Calculate compression ratio if applicable\n            if upload_info[2] > 0:\n                compression_ratio = (1 - (upload_info[3] / upload_info[2])) * 100\n                print(f\"‚Ä¢ Compression ratio: {compression_ratio:.1f}%\")\n        else:\n            print(\"‚ö†Ô∏è No upload details available\")\n            \n    except NameError:\n        print(\"\\n‚ö†Ô∏è Snowflake session not available - skipping stage creation and file upload\")\n    \n    # Final summary\n    print_section_header(\"üìä PIPELINE RESULTS SUMMARY üìä\")\n    print(f\"‚Ä¢ Training data shape: {training_processed.shape}\")\n    print(f\"‚Ä¢ Testing data shape: {testing_processed.shape}\")\n    print(f\"‚Ä¢ Memory usage: {cleaned_data.memory_usage().sum() / 1024**2:.2f} MB\")\n    \n    if 'session' in globals() or 'session' in locals():\n        print(f\"‚Ä¢ Pipeline saved to: @{stage_name}\")\n    else:\n        print(f\"‚Ä¢ Pipeline saved locally to: {PIPELINE_FILE}\")\n    \n    # Display data types in a cleaner format\n    print(\"\\nüìã DATA TYPES AFTER PROCESSING:\")\n    for col, dtype in zip(cleaned_data.columns, cleaned_data.dtypes):\n        print(f\"  ‚Ä¢ {col}: {dtype}\")\n    \n    print(\"\\n\" + \"‚ïê\"*70)\n    print(\"üéâ PIPELINE EXECUTION COMPLETED SUCCESSFULLY üéâ\".center(70))\n    print(\"‚ïê\"*70 + \"\\n\")\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a8c62d12-8d11-4797-8f9a-f89c2fa0b20e",
   "metadata": {
    "language": "sql",
    "name": "view_snowflake_implicit_bpr_recommender_processing_pipeline_stage",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "LS @snowflake_implicit_bpr_recommender_processing_pipeline_stage",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6f5849b5-a4ad-4eee-bfff-cd0d3a0f3ba6",
   "metadata": {
    "language": "python",
    "name": "data_cleaning",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "'''# Alternate to above pipeline code : Clean data by removing records without CustomerID and converting to correct type\nprint(\"\\n\" + \"=\"*50)\nprint(\"üßπ DATA CLEANING PROCESS\".center(50))\nprint(\"=\"*50)\n\n# Track initial state for reporting\ninitial_count = len(online_retail_data)\ninitial_customers = online_retail_data['CustomerID'].dropna().nunique()\n\n# Report initial status\nprint(f\"‚Ä¢ Initial dataset: {initial_count:,} transactions\")\nprint(f\"‚Ä¢ Found {online_retail_data['CustomerID'].isna().sum():,} transactions without CustomerID\")\n\n# Remove rows with missing CustomerID\nonline_retail_data = online_retail_data.dropna(subset=['CustomerID'])  # Filter out records without customer ID\nremoved_count = initial_count - len(online_retail_data)                # Calculate how many records were removed\n\n# Convert CustomerID to integer type\nonline_retail_data[\"CustomerID\"] = online_retail_data.CustomerID.astype(int)  # Ensure consistent ID format\n\n# Calculate result metrics\nremoval_percentage = (removed_count/initial_count) * 100\nfinal_customers = online_retail_data['CustomerID'].nunique()\n\n# Report detailed results with visual indicators\nprint(\"\\nüìä CLEANING RESULTS:\")\nprint(f\"‚úì Removed {removed_count:,} incomplete records ({removal_percentage:.1f}% of data)\")\nprint(f\"‚úì Converted CustomerID to integer type for consistency\")\nprint(f\"‚úì Final dataset contains {len(online_retail_data):,} valid transactions\")\nprint(f\"‚úì Dataset now has {final_customers:,} unique customers\")\n\n# Show data type summary after cleaning\nprint(\"\\nüìã CLEANED DATA TYPES:\")\nfor col, dtype in zip(online_retail_data.columns, online_retail_data.dtypes):\n    print(f\"  ‚Ä¢ {col}: {dtype}\")\n\n# Add a visual representation of the data cleaning effect\nprint(\"\\nüìà DATA CLEANING IMPACT:\")\nprint(\"Transactions:  \" + \"‚ñà\" * int(len(online_retail_data)/initial_count * 40) + \n      \"‚ñë\" * int(removed_count/initial_count * 40) + f\" {len(online_retail_data)/initial_count:.1%} retained\")\n\nprint(\"=\"*50)\n\n'''",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bd3ca60d-3cc2-4ff2-932f-166883c5d6a9",
   "metadata": {
    "name": "cell4",
    "collapsed": false
   },
   "source": "## üîç Dataset Dimension Analysis\n\nThis cell analyzes the core dimensions of our retail dataset, providing a quantitative overview of transactions, products, customers, and geographic distribution. Understanding these metrics helps calibrate our recommendation approach."
  },
  {
   "cell_type": "code",
   "id": "e58db344-eb5c-42dd-bd56-72d53bc4fd3d",
   "metadata": {
    "language": "python",
    "name": "dataset_dimension_analysis",
    "codeCollapsed": false,
    "collapsed": false
   },
   "outputs": [],
   "source": "# Analyze dataset dimensions and unique entities\nprint(\"\\n\" + \"=\"*60)\nprint(\"üîç RETAIL DATASET DIMENSION ANALYSIS\".center(60))\nprint(\"=\"*60)\n\n# Calculate key metrics\nmetrics = {\n    \"Transactions\": online_retail_data['Invoice'].nunique(),              # Count unique invoice numbers\n    \"Unique Products\": online_retail_data['StockCode'].nunique(),         # Count distinct product codes\n    \"Unique Customers\": online_retail_data['CustomerID'].nunique(),       # Count individual customers\n    \"Missing Customer IDs\": online_retail_data['CustomerID'].isnull().sum(),  # Count records with no customer ID\n    \"Countries\": online_retail_data['Country'].nunique(),                 # Count distinct countries\n    \"Date Range\": f\"{online_retail_data['InvoiceDate'].min().date()} to {online_retail_data['InvoiceDate'].max().date()}\"\n}\n\n# Display with enhanced formatting\nprint(\"\\nüìä CORE METRICS:\")\nprint(\"-\"*60)\nprint(f\"{'Category':<22} {'Value':<15} {'Notes'}\")\nprint(\"-\"*60)\n\n# Format and display each metric with additional context\nprint(f\"{'Transactions':<22} {metrics['Transactions']:,} {'(Unique invoice numbers)'}\")\nprint(f\"{'Unique Products':<22} {metrics['Unique Products']:,} {'(Distinct product codes)'}\")\nprint(f\"{'Unique Customers':<22} {metrics['Unique Customers']:,} {'(Individual buyers)'}\")\nprint(f\"{'Countries':<22} {metrics['Countries']} {'(Geographic distribution)'}\")\nprint(f\"{'Date Range':<22} {metrics['Date Range']} {'(Transaction period)'}\")\n\n# Calculate average transactions per customer\navg_trans = online_retail_data.groupby('CustomerID')['Invoice'].nunique().mean()\nprint(f\"{'Avg Trans/Customer':<22} {avg_trans:.1f} {'(Customer engagement metric)'}\")\n\n# Add top countries by transaction volume with visual indicators\ntop_countries = online_retail_data['Country'].value_counts().head(5)\ntotal_transactions = len(online_retail_data)\n\nprint(\"\\nüåç TOP 5 COUNTRIES BY TRANSACTION VOLUME:\")\nprint(\"-\"*60)\nprint(f\"{'Country':<15} {'Transactions':>12} {'Percentage':>12} {'Distribution'}\")\nprint(\"-\"*60)\n\nfor country, count in top_countries.items():\n    percentage = count / total_transactions * 100\n    bar_length = int(percentage / 2)  # Scale for visual bar\n    bar = '‚ñà' * bar_length\n    print(f\"{country:<15} {count:>12,} {percentage:>11.1f}% {bar}\")\n\n# Add product category insights (using first words of descriptions as proxy for categories)\nprint(\"\\nüì¶ PRODUCT CATEGORY INSIGHTS:\")\n# Extract first word as rough category and count frequencies\nonline_retail_data['Category'] = online_retail_data['Description'].str.split().str[0]\ntop_categories = online_retail_data['Category'].value_counts().head(5)\n\nprint(\"-\"*60)\nfor category, count in top_categories.items():\n    print(f\"‚Ä¢ {category:<15} {count:,} items\")\n\nprint(\"-\"*60)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0c6b96a8-0ad7-4e2b-8f7b-16af2ed1b144",
   "metadata": {
    "name": "cell21",
    "collapsed": false
   },
   "source": "## üìä Country Performance Analysis Dashboard\n\nThis cell creates a comprehensive visual dashboard that analyzes retail performance metrics across different countries. The dashboard combines revenue analysis, customer value assessment, time trends, and product category distribution into a single informative visualization."
  },
  {
   "cell_type": "code",
   "id": "6c53b342-1320-45a4-a87f-87eed52ecb9b",
   "metadata": {
    "language": "python",
    "name": "country_performance_dashboard",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "\"\"\"\nRetail Data Insights - Streamlined country performance analysis\n\"\"\"\n\ndef quick_retail_insights(df, top_n=6):\n    \"\"\"Generate comprehensive retail insights with minimal code\"\"\"\n    # Ensure required columns exist and calculate order values\n    if 'total_cost' not in df.columns:\n        df['total_cost'] = df['Quantity'] * df['Price']  # Calculate revenue per transaction\n    \n    # Print key metrics\n    print(f\"üìä RETAIL INSIGHTS | {len(df):,} transactions | {df['CustomerID'].nunique():,} customers | {df['Country'].nunique()} countries\")\n    print(f\"üí∞ Total Revenue: \\\\${df['total_cost'].sum():,.2f} | Avg Order: \\\\${df.groupby('Invoice')['total_cost'].sum().mean():,.2f}\")\n    \n    # Find cancelled transactions\n    cancelled = df[df['Invoice'].astype(str).str.startswith('C')]  # Identify cancelled orders (typically prefixed with 'C')\n    print(f\"‚ùå Cancelled Orders: {len(cancelled):,} ({len(cancelled)/len(df):.1%})\")\n    \n    # Country metrics - calculate key performance indicators by country\n    metrics = df.groupby(\"Country\").agg(\n        revenue=('total_cost', 'sum'),                      # Total revenue by country\n        avg_order=('total_cost', lambda x: x.mean()),       # Average order value\n        orders=('Invoice', 'nunique'),                      # Number of orders\n        customers=('CustomerID', 'nunique')                 # Unique customers\n    ).sort_values(by=\"revenue\", ascending=False)            # Sort by highest revenue\n    \n    metrics['share'] = metrics['revenue'] / metrics['revenue'].sum() * 100           # Calculate market share percentage\n    metrics['avg_customer_value'] = metrics['revenue'] / metrics['customers']        # Calculate customer lifetime value\n    top_countries = metrics.head(top_n)                                              # Select top performing countries\n    \n    # Create figure with optimized layout\n    fig = plt.figure(figsize=(16, 12))                      # Create large figure for dashboard\n    plt.style.use('seaborn-v0_8-whitegrid')                 # Set consistent styling\n    \n    # === COMBINED VISUALIZATION ===\n    # 1. Main revenue chart with integrated metrics\n    ax1 = plt.subplot2grid((2, 3), (0, 0), colspan=2)       # Create subplot in grid layout\n    \n    # Plot bars with custom styling\n    palette = sns.color_palette(\"viridis\", top_n)           # Generate color palette\n    bars = ax1.barh(top_countries.index, top_countries['revenue'], color=palette)  # Create horizontal bar chart\n    \n    # Add rich data labels\n    for i, bar in enumerate(bars):\n        country = top_countries.index[i]\n        revenue = top_countries['revenue'].iloc[i]\n        share = top_countries['share'].iloc[i]\n        customers = top_countries['customers'].iloc[i]\n        \n        # Add revenue and market share\n        ax1.text(\n            revenue + revenue*0.01, i,                      # Position text at end of bar\n            f\"\\\\${revenue:,.0f} ({share:.1f}%)\",            # Format revenue with \\$ and add percentage\n            va='center', fontweight='bold'                  # Center align vertically and make bold\n        )\n        \n        # Add customer count inside bar\n        if revenue > top_countries['revenue'].max() * 0.25:  # Only add text if bar is long enough\n            ax1.text(\n                revenue * 0.5, i,                            # Position text in middle of bar\n                f\"{customers:,} customers\",                  # Show customer count with comma formatting\n                va='center', ha='center',                    # Center align text\n                color='white', fontweight='bold'             # White text for visibility on colored bar\n            )\n    \n    ax1.set_title(\"Country Revenue Performance\", fontsize=14, fontweight='bold')\n    ax1.set_xlabel(\"Revenue (\\\\$)\", fontsize=12)\n    \n    # 2. Customer value vs order size scatter\n    ax2 = plt.subplot2grid((2, 3), (0, 2))                  # Create subplot for scatter plot\n    scatter = ax2.scatter(\n        top_countries['avg_order'],                         # X-axis: average order value\n        top_countries['avg_customer_value'],                # Y-axis: customer lifetime value \n        s=top_countries['orders']/10,                       # Size points by order count\n        c=range(len(top_countries)),                        # Color by revenue rank\n        cmap='viridis', alpha=0.8                           # Match color scheme with bar chart\n    )\n    \n    # Add country labels\n    for country in top_countries.index:\n        ax2.annotate(\n            country,                                         # Label points with country name\n            (top_countries.loc[country, 'avg_order'],        # X position\n             top_countries.loc[country, 'avg_customer_value']), # Y position\n            fontsize=9, fontweight='bold'                    # Make text readable\n        )\n    \n    ax2.set_title(\"Customer Value Analysis\", fontsize=14, fontweight='bold')\n    ax2.set_xlabel(\"Avg Order Value (\\\\$)\")\n    ax2.set_ylabel(\"Avg Customer Value (\\\\$)\")\n    ax2.grid(True, linestyle='--', alpha=0.7)               # Add subtle grid lines\n    \n    # 3. Time series for top 3 markets\n    ax3 = plt.subplot2grid((2, 3), (1, 0), colspan=2)       # Wide subplot for time series\n    \n    # Create monthly time series\n    df['month'] = pd.to_datetime(df['InvoiceDate']).dt.to_period('M').dt.to_timestamp()  # Convert dates to months\n    \n    # Plot top 3 countries\n    for i, country in enumerate(top_countries.index[:3]):    # Loop through top 3 countries\n        country_data = df[df['Country'] == country].groupby('month')['total_cost'].sum()  # Get monthly revenue\n        ax3.plot(\n            country_data.index, country_data.values,         # Plot time series\n            marker='o', linewidth=2, label=country, color=palette[i]  # Style with markers and matching colors\n        )\n        \n        # Add last value annotation\n        if not country_data.empty:\n            last_month = country_data.index[-1]              # Get most recent month\n            last_value = country_data.values[-1]             # Get final revenue value\n            ax3.annotate(\n                f\"\\\\${last_value:,.0f}\",                      # Format as currency\n                (last_month, last_value),                    # Position at end of line\n                xytext=(10, 0), textcoords='offset points'   # Offset slightly for readability\n            )\n    \n    ax3.set_title(\"Monthly Revenue Trends - Top Markets\", fontsize=14, fontweight='bold')\n    ax3.set_ylabel(\"Monthly Revenue (\\\\$)\")\n    ax3.legend()                                            # Add country legend\n    ax3.grid(True, alpha=0.3)                               # Add subtle grid\n    \n    # 4. Category mix by country (new insight)\n    ax4 = plt.subplot2grid((2, 3), (1, 2))                  # Create subplot for category analysis\n    \n    # Use product description first word as category proxy\n    df['category'] = df['Description'].astype(str).str.split().str[0]  # Extract first word as category\n    top_cats = df['category'].value_counts().head(5).index   # Get top 5 categories\n    \n    # Get category distribution for top 3 countries\n    cat_data = []\n    for country in top_countries.index[:3]:                  # For each top country\n        country_cats = df[df['Country'] == country]['category'].value_counts()  # Count categories\n        cat_data.append([country_cats.get(cat, 0) for cat in top_cats])  # Create list of category counts\n    \n    # Create normalized stacked bars\n    cat_data_norm = np.array(cat_data) / np.array(cat_data).sum(axis=1)[:, np.newaxis]  # Normalize to percentages\n    ax4.bar(top_countries.index[:3], np.ones(3), color='lightgray')  # Create background bars\n    bottom = np.zeros(3)                                     # Initialize bottom position\n    \n    for i, cat in enumerate(top_cats):                       # For each category\n        ax4.bar(\n            top_countries.index[:3], cat_data_norm[:, i],    # Plot proportional segment\n            bottom=bottom, label=cat, alpha=0.8              # Stack on previous segments\n        )\n        bottom += cat_data_norm[:, i]                        # Update bottom for next category\n    \n    ax4.set_title(\"Product Category Mix\", fontsize=14, fontweight='bold')\n    ax4.set_ylim(0, 1)                                       # Set y-axis from 0 to 100%\n    ax4.set_ylabel(\"Category Share\")\n    ax4.legend(fontsize=8)                                   # Add small legend\n    \n    # Add overall title and insights\n    plt.suptitle(\"Country Performance Dashboard\", fontsize=18, fontweight='bold', y=0.98)\n    \n    # Add insights box with key takeaways\n    insights = \"\\n\".join([\n        \"üìå \" + country + \": \" + \n        f\"\\\\${metrics.loc[country, 'revenue']:,.0f} revenue, \" + \n        f\"{metrics.loc[country, 'customers']:,} customers\"\n        for country in metrics.index[:3]                     # Create summary for top 3 countries\n    ])\n    \n    plt.figtext(0.5, 0.01, insights, ha='center', bbox={     # Add text box at bottom\n        'facecolor': 'lightyellow', 'alpha': 0.9, 'pad': 5,\n        'boxstyle': 'round,pad=0.5'                          # Style as a note\n    })\n    \n    plt.tight_layout(rect=[0, 0.05, 1, 0.95])                # Adjust layout for readability\n    return fig\n\n# Generate the dashboard\ndashboard = quick_retail_insights(online_retail_data)        # Create visualization with our data\nplt.show()   ",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2982a627-ae26-49b5-9bb0-2373cc2a650f",
   "metadata": {
    "name": "cell7",
    "collapsed": false
   },
   "source": "## ü§ñ Recommendation System Construction and Evaluation\n\nThis comprehensive cell implements a complete product recommendation engine using Bayesian Personalized Ranking (BPR). It includes data preparation, model training, evaluation metrics calculation, performance visualization, and a recommendation generator function for individual customers."
  },
  {
   "cell_type": "code",
   "id": "e2b93afd-9701-43f1-9932-c39c63e4365a",
   "metadata": {
    "language": "python",
    "name": "recommendation_system_builder",
    "codeCollapsed": false,
    "collapsed": false
   },
   "outputs": [],
   "source": "\"\"\"\nRetail Recommender System with Performance Metrics\n\"\"\"\n\ndef build_recommendation_system(df, test_size=0.2, factors=50, iterations=50, \n                              learning_rate=0.05, regularization=0.01):\n    \"\"\"Build and evaluate a recommendation system with comprehensive metrics\"\"\"\n    print(\"üìä RETAIL RECOMMENDER | Building recommendation system...\")\n    \n    # Track performance metrics\n    metrics = {}                                      # Dictionary to store all performance metrics\n    start_time = time.time()                          # Record start time for overall process\n    \n    # Step 1: Prepare data\n    print(\"1Ô∏è‚É£ Preparing data...\")\n    prep_start = time.time()                          # Track time for data preparation step\n    \n    # Focus on required columns with positive quantities\n    df_prep = df[['CustomerID', 'StockCode', 'Quantity']].copy()  # Extract relevant columns\n    df_prep = df_prep[(df_prep['CustomerID'].notna()) &           # Filter out rows with missing IDs\n                     (df_prep['StockCode'].notna()) &             # Filter out rows with missing products\n                     (df_prep['Quantity'] > 0)]                   # Only keep positive quantities (purchases)\n    \n    # Convert to strings for encoding\n    df_prep['CustomerID'] = df_prep['CustomerID'].astype(str)     # Ensure consistent formatting for encoding\n    df_prep['StockCode'] = df_prep['StockCode'].astype(str)       # Convert product codes to strings\n    \n    # Create encoders\n    user_encoder = LabelEncoder()                     # Encoder for customer IDs\n    item_encoder = LabelEncoder()                     # Encoder for product codes\n    \n    df_prep['user'] = user_encoder.fit_transform(df_prep['CustomerID'])  # Map customers to integer indices\n    df_prep['item'] = item_encoder.fit_transform(df_prep['StockCode'])   # Map products to integer indices\n    \n    # Record data metrics\n    metrics['total_users'] = len(user_encoder.classes_)            # Number of unique customers\n    metrics['total_items'] = len(item_encoder.classes_)            # Number of unique products\n    metrics['total_interactions'] = len(df_prep)                   # Total purchase records\n    metrics['data_prep_time'] = time.time() - prep_start           # Time taken for data preparation\n    \n    print(f\"   ‚úì Found {metrics['total_users']:,} customers and {metrics['total_items']:,} products\")\n    print(f\"   ‚úì Using {metrics['total_interactions']:,} purchase interactions\")\n    \n    # Step 2: Create train/test split for evaluation\n    print(\"2Ô∏è‚É£ Splitting data for evaluation...\")\n    split_start = time.time()                         # Track time for train/test splitting\n    \n    # Group by user-item for unique interactions\n    interactions = df_prep.groupby(['user', 'item'])['Quantity'].sum().reset_index()  # Aggregate purchases\n    \n    # Split at the user level for better evaluation\n    users = df_prep['user'].unique()                  # Get all unique user indices\n    train_users, test_users = train_test_split(users, test_size=test_size, random_state=42)  # Split users\n    \n    # Create train and test sets\n    train_df = df_prep[df_prep['user'].isin(train_users)]  # Training data from training users\n    test_df = df_prep[df_prep['user'].isin(test_users)]    # Test data from test users\n    \n    # Create matrices\n    n_users = df_prep['user'].max() + 1               # Total number of users (0-indexed)\n    n_items = df_prep['item'].max() + 1               # Total number of items (0-indexed)\n    \n    train_interactions = train_df.groupby(['user', 'item'])['Quantity'].sum().reset_index()  # Aggregate\n    train_matrix = coo_matrix(\n        (train_interactions['Quantity'].astype(float),                # Values (purchase quantities)\n         (train_interactions['user'], train_interactions['item'])),   # Row and column indices\n        shape=(n_users, n_items)                                      # Matrix dimensions\n    ).tocsr()                                         # Convert to CSR format for efficient operations\n    \n    test_interactions = test_df.groupby(['user', 'item'])['Quantity'].sum().reset_index()  # Aggregate\n    test_matrix = coo_matrix(\n        (test_interactions['Quantity'].astype(float),                 # Values (purchase quantities) \n         (test_interactions['user'], test_interactions['item'])),     # Row and column indices\n        shape=(n_users, n_items)                                      # Match train matrix dimensions\n    ).tocsr()                                         # Convert to CSR format for efficient operations\n    \n    metrics['split_time'] = time.time() - split_start  # Time taken for data splitting\n    print(f\"   ‚úì Created training set with {len(train_df):,} interactions\")\n    print(f\"   ‚úì Created test set with {len(test_df):,} interactions for evaluation\")\n    \n    # Step 3: Train model\n    print(\"3Ô∏è‚É£ Training recommendation model...\")\n    train_start = time.time()                         # Track time for model training\n    \n    model = BayesianPersonalizedRanking(\n        factors=factors,                              # Number of latent factors (complexity)\n        iterations=iterations,                        # Training iterations\n        learning_rate=learning_rate,                  # Step size for gradient descent\n        regularization=regularization,                # Regularization to prevent overfitting\n        random_state=42                               # Ensure reproducible results\n    )\n    model.fit(train_matrix.T)                         # Train on transposed matrix (implicit expects item-user format)\n    \n    metrics['training_time'] = time.time() - train_start  # Time taken for model training\n    print(f\"   ‚úì Model trained in {metrics['training_time']:.2f} seconds\")\n    print(f\"   ‚úì Using {factors} latent factors with {iterations} iterations\")\n    \n    # Step 4: Evaluate model\n    print(\"4Ô∏è‚É£ Evaluating model performance...\")\n    eval_start = time.time()                          # Track time for evaluation\n    \n    # Calculate precision@k and recall@k for test users\n    k_values = [5, 10, 20]                            # Different numbers of recommendations to evaluate\n    precision_at_k = {k: [] for k in k_values}        # Store precision for each k\n    recall_at_k = {k: [] for k in k_values}           # Store recall for each k\n    \n    # Sample users for evaluation (up to 100 for speed)\n    eval_users = np.random.choice(test_users, min(100, len(test_users)), replace=False)  # Sample users\n    \n    for user in eval_users:\n        # Get items this user has interacted with in test set\n        actual_items = set(test_df[test_df['user'] == user]['item'])  # Items the user actually purchased\n        if not actual_items:\n            continue                                  # Skip users with no test interactions\n            \n        # Get recommendations\n        try:\n            recs, scores = model.recommend(user, train_matrix[user], N=max(k_values))  # Generate recommendations\n        except:\n            continue                                  # Skip if recommendation fails\n            \n        # Calculate metrics\n        for k in k_values:\n            recommended_items = set(recs[:k])         # Top-k recommendations\n            relevant_and_recommended = len(recommended_items.intersection(actual_items))  # Hits\n            \n            # Precision = relevant & recommended / recommended\n            precision = relevant_and_recommended / k if k > 0 else 0  # Proportion of relevant recommendations\n            precision_at_k[k].append(precision)\n            \n            # Recall = relevant & recommended / relevant\n            recall = relevant_and_recommended / len(actual_items) if actual_items else 0  # Proportion of found items\n            recall_at_k[k].append(recall)\n    \n    # Calculate average metrics\n    for k in k_values:\n        metrics[f'precision@{k}'] = np.mean(precision_at_k[k]) if precision_at_k[k] else 0  # Average precision\n        metrics[f'recall@{k}'] = np.mean(recall_at_k[k]) if recall_at_k[k] else 0           # Average recall\n    \n    # Calculate coverage\n    all_recommended = set()                           # Track all items that get recommended\n    sample_users = np.random.choice(df_prep['user'].unique(), min(500, n_users), replace=False)  # Sample users\n    \n    for user in sample_users:\n        try:\n            recs, _ = model.recommend(user, train_matrix[user], N=10)  # Get recommendations for user\n            all_recommended.update(recs)                               # Add to set of recommended items\n        except:\n            continue                                  # Skip errors\n    \n    metrics['catalog_coverage'] = len(all_recommended) / n_items  # Proportion of catalog that gets recommended\n    metrics['eval_time'] = time.time() - eval_start               # Time taken for evaluation\n    \n    # Step 5: Save model\n    joblib.dump(model, \"snowflake_recommender_system_with_implicit_bpr_model.joblib\", compress=3)            # Save trained model\n    joblib.dump(user_encoder, \"user_encoder.joblib\", compress=3)              # Save user ID mapping\n    joblib.dump(item_encoder, \"item_encoder.joblib\", compress=3)              # Save item ID mapping\n    \n    metrics['total_time'] = time.time() - start_time                          # Total processing time\n    \n    # Print summary\n    print(\"\\n‚úÖ RECOMMENDATION SYSTEM BUILT SUCCESSFULLY\")\n    print(f\"Total processing time: {metrics['total_time']:.2f} seconds\")\n    \n    # Return all components\n    return {\n        'model': model,                               # Trained recommendation model\n        'user_encoder': user_encoder,                 # Mapping from customer IDs to indices\n        'item_encoder': item_encoder,                 # Mapping from product codes to indices\n        'train_matrix': train_matrix,                 # Training data in matrix form\n        'test_matrix': test_matrix,                   # Test data in matrix form\n        'metrics': metrics                            # Performance metrics\n    }\n\ndef visualize_model_performance(rec_system):\n    \"\"\"Create visualization of model performance metrics\"\"\"\n    metrics = rec_system['metrics']                   # Extract metrics from recommendation system\n    \n    # Create figure for metrics\n    plt.style.use('seaborn-v0_8-whitegrid')          # Clean visualization style\n    fig, axes = plt.subplots(2, 2, figsize=(16, 12)) # 2x2 grid of visualizations\n    \n    # 1. Precision and Recall Plot\n    ax1 = axes[0, 0]                                  # First subplot location\n    k_values = [5, 10, 20]                            # Different recommendation list sizes\n    precision_values = [metrics.get(f'precision@{k}', 0) for k in k_values]  # Get precision for each k\n    recall_values = [metrics.get(f'recall@{k}', 0) for k in k_values]        # Get recall for each k\n    \n    width = 0.35                                      # Bar width\n    x = np.arange(len(k_values))                      # Bar positions\n    ax1.bar(x - width/2, precision_values, width, label='Precision', color='#5DA5DA')  # Precision bars\n    ax1.bar(x + width/2, recall_values, width, label='Recall', color='#FAA43A')        # Recall bars\n    \n    ax1.set_xlabel('k (number of recommendations)')\n    ax1.set_ylabel('Recommendations Score')\n    ax1.set_title('Precision and Recall at k', fontweight='bold')\n    ax1.set_xticks(x)\n    ax1.set_xticklabels([f'k={k}' for k in k_values])\n    ax1.legend()\n    \n    # Add value labels\n    for i, v in enumerate(precision_values):\n        ax1.text(i - width/2, v + 0.01, f'{v:.3f}', ha='center')  # Label precision values\n    for i, v in enumerate(recall_values):\n        ax1.text(i + width/2, v + 0.01, f'{v:.3f}', ha='center')  # Label recall values\n    \n    # 2. Coverage and User/Item Stats\n    ax2 = axes[0, 1]                                  # Second subplot location\n    coverage = metrics.get('catalog_coverage', 0)     # Get catalog coverage metric\n    \n    # Create gauge-style chart for coverage\n    coverage_pct = coverage * 100                     # Convert to percentage\n    gauge = ax2.pie([coverage_pct, 100-coverage_pct], \n                   startangle=90, colors=['#60BD68', '#EEEEEE'],    # Green for covered, gray for uncovered\n                   wedgeprops=dict(width=0.3, edgecolor='w'))       # Donut chart style\n    \n    # Add labels and annotations\n    ax2.text(0, 0, f\"{coverage_pct:.1f}%\", ha='center', va='center', fontsize=24, fontweight='bold')  # Center text\n    ax2.text(0, -1.5, \"Catalog Coverage\", ha='center', fontsize=14) # Label below chart\n    ax2.set_title('Model Coverage', fontweight='bold')\n    \n    # 3. Training Performance\n    ax3 = axes[1, 0]                                  # Third subplot location\n    \n    # Time metrics\n    time_metrics = [\n        ('Data Prep', metrics.get('data_prep_time', 0)),   # Time for data preparation\n        ('Split', metrics.get('split_time', 0)),           # Time for train/test split\n        ('Training', metrics.get('training_time', 0)),     # Time for model training\n        ('Evaluation', metrics.get('eval_time', 0))        # Time for model evaluation\n    ]\n    \n    # Create horizontal bars for timing\n    y_pos = np.arange(len(time_metrics))              # Bar positions\n    times = [t[1] for t in time_metrics]              # Time values\n    labels = [t[0] for t in time_metrics]             # Step labels\n    \n    bars = ax3.barh(y_pos, times, color=plt.cm.viridis(np.linspace(0, 0.8, len(times))))  # Colorful bars\n    \n    # Add time labels\n    for i, v in enumerate(times):\n        ax3.text(v + 0.1, i, f'{v:.2f}s', va='center')  # Label with seconds\n    \n    ax3.set_yticks(y_pos)\n    ax3.set_yticklabels(labels)\n    ax3.set_xlabel('Time (seconds)')\n    ax3.set_title('Processing Time Breakdown', fontweight='bold')\n    \n    # 4. Data Statistics\n    ax4 = axes[1, 1]                                  # Fourth subplot location\n    \n    # Prepare data stats\n    data_stats = {\n        'Customers': metrics.get('total_users', 0),         # Number of unique customers\n        'Products': metrics.get('total_items', 0),          # Number of unique products\n        'Interactions': metrics.get('total_interactions', 0) # Total purchase records\n    }\n    \n    # Create horizontal bars with log scale for better visualization\n    labels = list(data_stats.keys())                  # Category labels\n    values = list(data_stats.values())                # Count values\n    \n    y_pos = np.arange(len(labels))                    # Bar positions\n    bars = ax4.barh(y_pos, values, color=plt.cm.viridis(np.linspace(0.2, 1, len(values))))  # Colorful bars\n    \n    # Add value labels\n    for i, v in enumerate(values):\n        ax4.text(v + v*0.01, i, f'{v:,}', va='center')  # Label with formatted numbers\n    \n    ax4.set_yticks(y_pos)\n    ax4.set_yticklabels(labels)\n    ax4.set_xscale('log')                             # Log scale to handle different magnitudes\n    ax4.set_title('Dataset Statistics', fontweight='bold')\n    \n    # Add overall title\n    plt.suptitle('Recommendation System Performance Metrics', fontsize=20, fontweight='bold', y=0.98)\n    \n    # Add model specs as text box\n    model_specs = \"\\n\".join([\n        f\"‚Ä¢ Algorithm: Bayesian Personalized Ranking\",      # Model type\n        f\"‚Ä¢ Latent Factors: {rec_system['model'].factors}\", # Complexity parameter\n        f\"‚Ä¢ Learning Rate: {rec_system['model'].learning_rate}\", # Training parameter\n        f\"‚Ä¢ Training Iterations: {rec_system['model'].iterations}\", # Training thoroughness\n        f\"‚Ä¢ Regularization: {rec_system['model'].regularization}\"   # Overfitting control\n    ])\n    \n    plt.figtext(0.5, 0.01, model_specs, ha='center', bbox={  # Add text box at bottom\n        'facecolor': '#F5F5F5', 'alpha': 0.9, 'pad': 5,\n        'boxstyle': 'round,pad=0.5'\n    })\n    \n    plt.tight_layout(rect=[0, 0.05, 1, 0.95])         # Adjust spacing for readability\n    return fig\n\ndef recommend_for_customer(customer_id, rec_system, num_recs=5):\n    \"\"\"Generate recommendations for a specific customer with visualization\"\"\"\n    model = rec_system['model']                        # Trained recommendation model\n    user_encoder = rec_system['user_encoder']          # Customer ID mapping\n    item_encoder = rec_system['item_encoder']          # Product code mapping\n    train_matrix = rec_system['train_matrix']          # User-item interaction matrix\n    \n    # Check if customer exists\n    if customer_id not in user_encoder.classes_:\n        print(f\"‚ö†Ô∏è Customer {customer_id} not found. Showing popular items instead.\")\n        # Get popular items\n        item_popularity = np.asarray(train_matrix.sum(axis=0)).flatten()  # Sum interactions by item\n        popular_items = np.argsort(-item_popularity)[:num_recs]  # Sort by popularity (descending)\n        \n        # Create recommendation dataframe\n        recs_df = pd.DataFrame({\n            'StockCode': item_encoder.classes_[popular_items],  # Convert indices to product codes\n            'Score': np.linspace(0.5, 0.3, num_recs),          # Generate declining scores\n            'Type': 'Popular Item'                              # Indicate recommendation type\n        })\n        return recs_df\n    \n    # Get user index\n    user_idx = np.where(user_encoder.classes_ == customer_id)[0][0]  # Look up customer index\n    \n    # Get user's purchased items\n    user_items = train_matrix[user_idx]               # Extract user's row from interaction matrix\n    \n    try:\n        # Get recommendations with safety checks for index bounds\n        recommended_items, scores = model.recommend(\n            user_idx, user_items, N=num_recs,         # Request specific number of recommendations\n            filter_already_liked_items=True            # Don't recommend already purchased items\n        )\n        \n        # Make sure indices are within bounds\n        valid_mask = recommended_items < len(item_encoder.classes_)  # Check for valid product indices\n        if not all(valid_mask):\n            print(f\"‚ö†Ô∏è Some recommendations were out of bounds. Filtering to valid items only.\")\n            recommended_items = recommended_items[valid_mask]  # Keep only valid indices\n            scores = scores[valid_mask]                        # Keep corresponding scores\n        \n        # Create recommendation dataframe\n        recs_df = pd.DataFrame({\n            'StockCode': item_encoder.classes_[recommended_items],  # Convert indices to product codes\n            'Score': scores,                                        # Include predicted scores\n            'Type': 'Personalized'                                  # Indicate recommendation type\n        })\n        \n    except Exception as e:\n        print(f\"‚ö†Ô∏è Error generating recommendations: {str(e)}\")\n        print(\"Showing popular items instead.\")\n        \n        # Fall back to popular items\n        item_popularity = np.asarray(train_matrix.sum(axis=0)).flatten()  # Calculate item popularity\n        popular_indices = np.argsort(-item_popularity)                    # Sort by popularity\n        \n        # Filter to ensure indices are within bounds\n        valid_indices = [i for i in popular_indices if i < len(item_encoder.classes_)][:num_recs]\n        \n        # Create recommendation dataframe with popular items\n        recs_df = pd.DataFrame({\n            'StockCode': item_encoder.classes_[valid_indices],  # Convert indices to product codes\n            'Score': np.linspace(0.5, 0.3, len(valid_indices)), # Generate declining scores\n            'Type': 'Popular Item (Fallback)'                   # Indicate fallback recommendation\n        })\n    \n    return recs_df\n\n# Run the full pipeline\nprint(\"Building and evaluating recommendation system...\")\nrecommendation_system = build_recommendation_system(online_retail_data)  # Build complete recommendation system\n\n# Visualize performance\nprint(\"\\nGenerating performance visualizations...\")\nperformance_vis = visualize_model_performance(recommendation_system)  # Create performance dashboard\nplt.show()                                                            # Display visualization\n\n# Generate sample recommendations\nprint(\"\\nSample recommendations for a customer:\")\nsample_customer = recommendation_system['user_encoder'].classes_[0]  # Get first customer from dataset\nrecommendations = recommend_for_customer(sample_customer, recommendation_system, num_recs=5)  # Generate recommendations\ndisplay(recommendations)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8f91d2b9-b803-462f-b33f-bd78031a8667",
   "metadata": {
    "name": "cell23",
    "collapsed": false
   },
   "source": "## üõçÔ∏è Customer Product Recommendation Function\n\nThis enhanced recommendation function generates personalized product suggestions for any customer, adding detailed product descriptions from the original dataset and handling various edge cases for improved reliability."
  },
  {
   "cell_type": "code",
   "id": "63e19cb6-a0aa-4383-bc56-e25989138043",
   "metadata": {
    "language": "python",
    "name": "customer_recommendation_generator",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "def recommend_for_customer(customer_id, rec_system, df, num_recs=5):\n    \"\"\"Generate recommendations for a specific customer with visualization\"\"\"\n    model = rec_system['model']                        # Access the trained recommendation model\n    user_encoder = rec_system['user_encoder']          # Mapping between customer IDs and internal indices\n    item_encoder = rec_system['item_encoder']          # Mapping between product codes and internal indices\n    train_matrix = rec_system['train_matrix']          # User-item interaction matrix from training data\n    \n    # Check if customer exists in our training data\n    if customer_id not in user_encoder.classes_:\n        print(f\"‚ö†Ô∏è Customer {customer_id} not found. Showing popular items instead.\")\n        # Get popular items as fallback recommendation strategy\n        item_popularity = np.asarray(train_matrix.sum(axis=0)).flatten()  # Sum interactions by item\n        popular_items = np.argsort(-item_popularity)[:num_recs]           # Get top-N most popular items\n        \n        # Create recommendation dataframe with popular items\n        recs_df = pd.DataFrame({\n            'StockCode': item_encoder.classes_[popular_items],            # Map indices back to product codes\n            'Score': np.linspace(0.5, 0.3, num_recs),                     # Assign declining confidence scores\n            'Type': 'Popular Item'                                        # Mark as popularity-based recommendation\n        })\n        \n        # Add customer ID to all recommendations\n        recs_df['CustomerID'] = customer_id                               # Include target customer ID\n        \n        # Merge with original dataframe to get descriptions\n        # Get unique stock codes and descriptions\n        stock_desc_df = df[['StockCode', 'Description']].drop_duplicates()  # Create lookup table\n        recs_df = recs_df.merge(stock_desc_df, on='StockCode', how='left')  # Join to get descriptions\n        \n        # Reorganize columns\n        recs_df = recs_df[['CustomerID', 'StockCode', 'Description', 'Type', 'Score']]\n        \n        return recs_df\n    \n    # Get user index for existing customers\n    user_idx = np.where(user_encoder.classes_ == customer_id)[0][0]       # Convert customer ID to matrix index\n    \n    # Get user's purchased items\n    user_items = train_matrix[user_idx]                                   # Extract user's interaction profile\n    \n    try:\n        # Get recommendations with safety checks for index bounds\n        recommended_items, scores = model.recommend(\n            user_idx, user_items, N=num_recs,                             # Request specific number of recommendations\n            filter_already_liked_items=True                               # Don't recommend items already purchased\n        )\n        \n        # Make sure indices are within bounds\n        valid_mask = recommended_items < len(item_encoder.classes_)        # Check for valid indices\n        if not all(valid_mask):\n            print(f\"‚ö†Ô∏è Some recommendations were out of bounds. Filtering to valid items only.\")\n            recommended_items = recommended_items[valid_mask]              # Filter to valid indices\n            scores = scores[valid_mask]                                    # Filter corresponding scores\n        \n        # Create recommendation dataframe\n        recs_df = pd.DataFrame({\n            'StockCode': item_encoder.classes_[recommended_items],         # Map indices to product codes\n            'Score': scores,                                               # Include confidence scores\n            'Type': 'Personalized'                                         # Mark as personalized recommendations\n        })\n        \n    except Exception as e:\n        print(f\"‚ö†Ô∏è Error generating recommendations: {str(e)}\")\n        print(\"Showing popular items instead.\")\n        \n        # Fall back to popular items if personalization fails\n        item_popularity = np.asarray(train_matrix.sum(axis=0)).flatten()   # Calculate popularity\n        popular_indices = np.argsort(-item_popularity)                     # Sort by popularity\n        \n        # Filter to ensure indices are within bounds\n        valid_indices = [i for i in popular_indices if i < len(item_encoder.classes_)][:num_recs]\n        \n        # Create recommendation dataframe with popular items\n        recs_df = pd.DataFrame({\n            'StockCode': item_encoder.classes_[valid_indices],             # Map indices to product codes\n            'Score': np.linspace(0.5, 0.3, len(valid_indices)),            # Assign declining confidence scores\n            'Type': 'Popular Item (Fallback)'                              # Mark as fallback recommendations\n        })\n    \n    # Add customer ID to all recommendations\n    recs_df['CustomerID'] = customer_id                                    # Include target customer ID\n    \n    # Merge with original dataframe to get descriptions\n    # Get unique stock codes and descriptions\n    stock_desc_df = df[['StockCode', 'Description']].drop_duplicates()     # Create product lookup table\n    recs_df = recs_df.merge(stock_desc_df, on='StockCode', how='left')     # Join to get descriptions\n    \n    # Reorganize columns to display in desired order\n    recs_df = recs_df[['CustomerID', 'StockCode', 'Description', 'Type', 'Score']]\n    \n    return recs_df\n\n# Generate sample recommendations\nsample_customer = recommendation_system['user_encoder'].classes_[0]           # Get first customer ID from dataset\nprint(\"\\nSample recommendations for a customer:\" + sample_customer)           # Display customer ID\nrecommendations = recommend_for_customer(sample_customer, recommendation_system, online_retail_data, num_recs=5)  # Generate recommendations\ndisplay(recommendations) ",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ab1f3536-2c96-4814-8572-85aab56c531d",
   "metadata": {
    "name": "cell9",
    "collapsed": false
   },
   "source": "## üìú Customer Purchase History Analysis\n\nThis cell retrieves and displays the actual purchase history for a specific customer, providing the foundation for comparing and evaluating our recommendation system's suggestions against known preferences."
  },
  {
   "cell_type": "code",
   "id": "d6f45f1e-cd44-486c-b54b-5ae3a8c0a3de",
   "metadata": {
    "language": "python",
    "name": "customer_purchase_history",
    "codeCollapsed": false,
    "collapsed": false
   },
   "outputs": [],
   "source": "# Take Customer ID: 13085 for our analysis from the original dataset\nsample_customer_id = 13085                                            # Select a specific customer for analysis\n# Let us present the top 10 original purchase history for this customer.\ndisplay_original_purchase_history(online_retail_data, sample_customer_id, 10)  # Show customer's actual purchases",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5b3ae232-ef71-4910-bd87-dea1b85564f4",
   "metadata": {
    "name": "cell14",
    "collapsed": false
   },
   "source": "## üéØ Real-Time Product Recommendations\n\nThis cell generates personalized product recommendations for customer 13085 in real-time, using our trained recommendation model. The results showcase the model's ability to suggest relevant products based on past purchase patterns."
  },
  {
   "cell_type": "code",
   "id": "02a6f485-89aa-430b-9fab-042a4c0e69e6",
   "metadata": {
    "language": "python",
    "name": "customer_recommendations",
    "codeCollapsed": false,
    "collapsed": false
   },
   "outputs": [],
   "source": "# For real-time recommendations for customer 13085\n#result = display_inference_result(recommendations, 13085, \"realtime\", num_results=15)\n#display(result)  # Add this line\n\n# Generate sample recommendations\nsample_customer = '13085'                                                  # Target customer ID for recommendations\nprint(\"\\nSample recommendations for a customer:\" + sample_customer)        # Display customer identifier\nrecommendations = recommend_for_customer(sample_customer,                  # Generate personalized recommendations\n                                        recommendation_system,             # Using our trained model\n                                        online_retail_data,                # And complete dataset for descriptions\n                                        num_recs=20)                        # Request 6 recommendations\ndisplay(recommendations)  ",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5ea4f974-d6aa-4466-bd62-820a99ab39b3",
   "metadata": {
    "name": "cell10",
    "collapsed": false
   },
   "source": "## üìã Recommendation Display Utility Function\n\nThis utility function creates a beautifully formatted display of product recommendations for a specific customer. It handles column mapping, adds product descriptions, applies visual styling, and works in different inference contexts."
  },
  {
   "cell_type": "code",
   "id": "2fe405fb-e044-4df4-83c1-2824b73ecc29",
   "metadata": {
    "language": "python",
    "name": "recommendation_display_formatter",
    "codeCollapsed": false,
    "collapsed": false
   },
   "outputs": [],
   "source": "'''\ndef display_inference_result(inference_result_df, customer_id, inference_type, num_results=10):\n    \"\"\"\n    Display recommendation results in a beautiful table format with customer ID\n    \n    Args:\n        inference_result_df: DataFrame with recommendation results\n        customer_id: ID of the customer\n        inference_type: Either 'batch' or 'realtime'\n        num_results: Number of results to display (default: 10)\n    \"\"\"\n    # Make a copy to avoid modifying the original\n    result = inference_result_df.copy()                                 # Work with a copy to preserve original data\n    \n    # Check how many recommendations we actually have\n    actual_count = len(result)                                          # Count available recommendations\n    if actual_count < num_results:\n        print(f\"Note: Only {actual_count} recommendations available (requested {num_results})\")\n    \n    # Rename columns to standard names if needed\n    column_mapping = {\n        \"user_id\": \"CustomerID\",                                        # Standardize customer ID column\n        \"item_id\": \"StockCode\",                                         # Standardize product code column\n        \"score\": \"Recommendation Score\"                                 # Standardize score column\n    }\n    \n    # Only rename columns that exist\n    rename_dict = {}\n    for old_col, new_col in column_mapping.items():\n        if old_col in result.columns:                                   # Check if column exists before renaming\n            rename_dict[old_col] = new_col\n    \n    if rename_dict:\n        result = result.rename(columns=rename_dict)                     # Apply column renaming\n    \n    # FIXED: Always set CustomerID to the value provided in the parameter\n    result[\"CustomerID\"] = customer_id                                  # Ensure customer ID is consistent\n    \n    # Convert StockCode to string\n    if \"StockCode\" in result.columns:\n        result[\"StockCode\"] = result[\"StockCode\"].astype(str)           # Ensure consistent string format for product codes\n    \n    # Add descriptions using a simpler approach\n    if \"StockCode\" in result.columns:\n        try:\n            # Create a mapping dictionary for descriptions\n            desc_map = dict(zip(\n                stock_code_desc_look_up[\"StockCode\"].astype(str),       # Convert keys to strings for matching\n                stock_code_desc_look_up[\"Description\"]                  # Use product descriptions as values\n            ))\n            \n            # Add descriptions column\n            result[\"Description\"] = result[\"StockCode\"].map(desc_map)   # Map product codes to descriptions\n        except Exception as e:\n            print(f\"Note: Could not add descriptions - {e}\")            # Handle errors gracefully\n    \n    # Reorder columns to ensure CustomerID is first\n    cols = [\"CustomerID\"]                                               # Start with customer ID\n    for col in result.columns:\n        if col != \"CustomerID\":\n            cols.append(col)                                            # Add all other columns\n    result = result[cols]                                               # Reorder columns\n    \n    # Limit to requested number of results\n    result = result.head(num_results)                                   # Trim to requested number\n    \n    # Print appropriate header\n    display_count = min(num_results, actual_count)                      # Use actual count if less than requested\n    if inference_type == \"batch\":\n        print(f\"\\n[ BATCH TRANSFORM ] Top {display_count} Recommendations for Customer ID: {customer_id}\")\n    else:\n        print(f\"\\n[ REAL-TIME INFERENCE ] Top {display_count} Recommendations for Customer ID: {customer_id}\")\n    \n    # Try to apply styling if in a notebook environment\n    try:\n        from IPython.display import display                             # Import notebook display functionality\n        import pandas as pd                                             # Ensure pandas is available\n        \n        # Apply styling for a beautiful table\n        styled_result = result.style.set_properties(**{\n            'background-color': '#f5f5f5',                              # Light gray background\n            'border-color': '#888888',                                  # Medium gray border\n            'border-style': 'solid',                                    # Solid border style\n            'border-width': '1px',                                      # 1px border width\n            'border-collapse': 'collapse',                              # Collapse borders\n            'padding': '8px',                                           # Comfortable padding\n            'font-size': '11pt'                                         # Readable font size\n        })\n        \n        # Format recommendation scores\n        if \"Recommendation Score\" in result.columns:\n            styled_result = styled_result.format({\n                'Recommendation Score': '{:.4f}'                         # Format scores to 4 decimal places\n            })\n            \n            # Add color gradient to recommendation scores\n            styled_result = styled_result.background_gradient(\n                cmap='Blues',                                           # Blue color gradient\n                subset=['Recommendation Score']                          # Apply only to score column\n            )\n        \n        # Hide index\n        try:\n            styled_result = styled_result.hide_index()                  # Hide index numbers for cleaner display\n        except:\n            pass                                                        # Gracefully handle older pandas versions\n        \n        # Display the styled table\n        display(styled_result)                                          # Show the formatted table\n        return result\n    \n    except ImportError:\n        # Fall back to regular display if not in a notebook\n        print(\"\\nRecommendation Results:\")\n        print(result.to_string(index=False))                            # Plain text fallback\n        return result\n\n'''",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9312d140-5f35-4fb5-bc00-9d08aee86c40",
   "metadata": {
    "language": "python",
    "name": "cell1",
    "codeCollapsed": false,
    "collapsed": false
   },
   "outputs": [],
   "source": "",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f9fdad65-f874-4332-acf8-846bc35b4109",
   "metadata": {
    "language": "python",
    "name": "cell15",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6b72d3b8-5fe3-456f-b6b4-a497d06db60c",
   "metadata": {
    "language": "python",
    "name": "cell17",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "",
   "execution_count": null
  }
 ]
}